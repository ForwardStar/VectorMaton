# Vectormaton
An elegant index that supports hybrid queries of ANNs whose associated strings contain a queried substring. Each data in the vector database consists of a string and a vector. Each query contains a string, a vector, and an integer k to return approximated k-nearest neighbors. The query results will contain data that involves the queried string as a substring, and its vector is an approximated k-nearest neighbor of the queried vector under the substring constraint. In this project, we use Euclidean distance as the measure of closeness, but it can be simply extended to support other metrics.

Example scenario:
- In bioinformatics, each protein can be represented by (ùë†,ùë£), where ùë† is its amino acid sequence (e.g., Leu-Ser-Met) and ùë£ is its 2D or 3D structural embedding (e.g., AlphaFold embeddings);

- Query: searching the k-most similar protein structures containing a specific motif (i.e., a substring of amino acid sequence).

Example query:
- There are 4 (ùë†,ùë£) pairs in the vector database: ("ATP synthase subunit beta", [0, 1, 2]), ("ATP-dependent helicase", [3, 4, 5]), ("DNA polymerase III", [6, 7, 8]), ("Lactate dehydrogenase", [9, 10, 11])
- Query 1: k=2, v=[5, 6, 7], s=""
- Result 1: ("ATP-dependent helicase", [3, 4, 5]), ("DNA polymerase III", [6, 7, 8])
- To be more specific, this returns the 2-NNs of vector v with an empty substring constraint, which is equalalent to the classic ANN search;
- Query 2: k=2, v=[5, 6, 7], s="ATP"
- Result 2: ("ATP synthase subunit beta", [0, 1, 2]), ("ATP-dependent helicase", [3, 4, 5])
- This gives a substring constraint "ATP"; therefore, only data containing "ATP" as a substring will be considered.

# Compile and run
Parts of the project depend on ``openssl``. Install on Ubuntu:
```sh
sudo apt-get install libssl-dev
```

We developed and tested this vector database under ``GCC 10.5.0`` with ``O3`` optimization. To compile the codes, simply run:
```sh
mkdir build && cd build
cmake ..
make
```

This will generate executable files ``nsw_test``, ``hnsw_test``, ``sa_test``, ``baseline_test``, ``vectormaton_test`` and ``main``. In particular, ``vectormaton_test`` corresponds to ``source/test_vectormaton.cpp``, which provides a demo on how to use the index.

The ``main`` is our experimental program. Run with:
```sh
./main <string_data_file> <vector_data_file> <string_query_file> <vector_query_file> <k_query_file> <output_file> <Exact|Baseline|VectorMaton-full|VectorMaton-smart>
```

To show debug messages, add ``--debug`` option when executing the ``main`` program. To limit the number of vectors and strings inserted, add ``--data-size=<n>`` to only select the first n vectors and strings of the data file.

# Computing recall
To compute the average recall, run:
```sh
python3 compute_recall.py <exact_output_file> <actual_output_file>
```

# Datasets
Since there are no existing vector datasets associated with strings, we include synthetic datasets for experiments, where the strings are original natural language texts and the vectors are their embeddings generated by pre-trained language models. The datasets include:
- [CodeSearchNet](https://huggingface.co/datasets/irds/codesearchnet): a dataset of code snippets and their embeddings; to be specific, each data consists of a function name as its string and a code embedding vector (generated by CodeBERT) as its vector;
- [SwissProt]():
- [ArXiv]():

We provide a Python script ``generate_datasets.py`` to download and generate aforementioned datasets. The generated datasets are stored in the ``datasets/`` folder. Each dataset contains a file ``vectors.txt`` and a file ``strings.txt``, where the i-th line of ``vectors.txt`` and the i-th line of ``strings.txt`` correspond to the vector and string of the i-th data, respectively. To execute the script, please first install the required ``datasets``, ``numpy``, ``transformers`` and ``torch`` packages via:
```sh
pip install datasets==3.6.0 numpy==1.26.4 transformers==4.56.0 torch==2.8.0
```

Note that: at the time of our development, some datasets are not compatible with ``datasets`` library version later than 3.6.0, so please make sure to install the exact version above.

Then run:
```sh
python3 generate_datasets.py
```

Note that the script may take hours to finish, as it needs to download the dataset and compute the embeddings. The script does not support checkpointing for single dataset, so if it is interrupted, you need to delete that dataset folder and restart it. The script will skip the dataset generation if the dataset already exists.

# Queries
Generate queries by:
```sh
python3 generate_queries.py
```
and input the selected datasets, queried string length, etc. The queried strings are randomly sampled from the substrings of the original dataset. The queried vectors are randomly generated containing floating numbers. Generated quries are written into ``strings.txt``, ``vectors.txt`` and ``k.txt``.