# VectorMaton
An elegant index that supports hybrid queries of ANNs whose associated strings contain a queried substring. Each data in the vector database consists of a string and a vector. Each query contains a string, a vector, and an integer k to return approximated k-nearest neighbors. The query results will contain data that involves the queried string as a substring, and its vector is an approximated k-nearest neighbor of the queried vector under the substring constraint. In this project, we use Euclidean distance as the measure of closeness, but it can be simply extended to support other metrics.

# Compile and run
Parts of the project depend on ``openssl``. Install on Ubuntu:
```sh
sudo apt-get install libssl-dev
```

Fetch the ``hnswlib`` submodule:
```sh
git submodule update --init --recursive
```

We developed and tested this vector database under ``GCC 10.5.0`` with ``O3`` optimization and ``OpenMP``. To compile the codes, simply run:
```sh
mkdir build && cd build
cmake ..
make
```

This will generate executable files ``nsw_test``, ``hnsw_test``, ``sa_test``, ``vectormaton_test`` and ``main``. In particular, ``vectormaton_test`` corresponds to ``source/test_vectormaton.cpp``, which provides a demo on how to use the index.

The ``main`` is our experimental program. Run with:
```sh
./main <string_data_file> <vector_data_file> <string_query_file> <vector_query_file> <k_query_file> <PreFiltering|PostFiltering|VectorMaton-full|VectorMaton-smart|VectorMaton-parallel>
```

It will output recall and time consumption statistics of the corresponding method. To show debug messages, add ``--debug`` option when executing the ``main`` program. To limit the number of vectors and strings inserted, add ``--data-size=<n>`` to only select the first n vectors and strings of the data file. To write statistics to a csv file, add ``--statistics-file=output_statistics.csv`` to output the info to ``output_statistics.csv``. Add ``--load-index=index_files_folder`` to load index from disk, add ``--save-index=index_files_folder`` to save the index to disk. Add ``--num-threads=...`` when using ``VectorMaton-parallel``. Add ``--write-ground-truth=ground_truth.txt`` to write ground truth results to ``ground_truth.txt``.

# Datasets
Since there are no existing vector datasets associated with strings, we include synthetic datasets for experiments, where the strings are original natural language texts and the vectors are their embeddings generated by pre-trained language models. The datasets include:
- [CodeSearchNet](https://huggingface.co/datasets/irds/codesearchnet): a dataset of code snippets and their embeddings; to be specific, each data consists of a function name as its string and a code embedding vector (generated by CodeBERT) as its vector;
- [SwissProt](https://huggingface.co/datasets/khairi/uniprot-swissprot): a manually curated section of the UniProt protein sequence database; to be specific, each data consists of a protein sequence and its structural embedding vector (generated by ProtBERT) as its vector;
- [ArXiv](https://huggingface.co/datasets/Qdrant/arxiv-titles-instructorxl-embeddings): a dataset of paper titles and their embeddings; to be specific, each data consists of a paper title as its string and a text embedding vector (generated by InstructorXL) as its vector;
- [ArXiv-Small](https://huggingface.co/datasets/malteos/aspect-paper-embeddings): a dataset of paper titles and their embeddings; to be specific, each data consists of a paper title as its string and a text embedding vector (generated by all-mpnet-base-v2) as its vector.

We provide a Python script ``generate_datasets.py`` to download and generate aforementioned datasets. The generated datasets are stored in the ``datasets/`` folder. Each dataset contains a file ``vectors.txt`` and a file ``strings.txt``, where the i-th line of ``vectors.txt`` and the i-th line of ``strings.txt`` correspond to the vector and string of the i-th data, respectively. To execute the script, please first install the required ``datasets``, ``numpy``, ``transformers`` and ``torch`` packages via:
```sh
pip install datasets==3.6.0 numpy==1.26.4 transformers==4.56.0 torch==2.8.0
```

Note that: at the time of our development, some datasets are not compatible with ``datasets`` library version later than 3.6.0, so please make sure to install the exact version above.

Then run:
```sh
python3 generate_datasets.py
```

Note that the script may take hours to finish, as it needs to download the dataset and compute the embeddings. The script does not support checkpointing for single dataset, so if it is interrupted, you need to delete that dataset folder and restart it. The script will skip the dataset generation if the dataset already exists.

# Queries
Generate queries by:
```sh
python3 generate_queries.py
```
and input the selected datasets, queried string length, etc. The queried strings are randomly sampled from the substrings of the original dataset. The queried vectors are randomly generated containing floating numbers. Generated quries are written into ``strings.txt``, ``vectors.txt`` and ``k.txt``.

# Running example
Following is a minimal running example.

Firstly, compile the project:
```sh
> git submodule update --init --recursive
> mkdir build && cd build
> cmake ..
> make
```

Then download datasets by:
```sh
> python3 generate_datasets.py
```

Then generate query data:
```sh
> python3 generate_queries.py
Available datasets:
0: arxiv-small
1: swissprot
2: code_search_net
3: arxiv
Enter the index of the dataset to use: 0
Enter the desired string length for queries: 3
Enter the number of queries to generate: 1000
Enter value k for k-NN search: 10
Enter the number of elements you want to select from the dataset (-1 for all): -1
```

Finally, run PreFiltering on the query data:
```sh
> ./build/main datasets/arxiv-small/strings.txt datasets/arxiv-small/vectors.txt strings.txt vectors.txt k.txt PreFiltering
```